# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.htmlfrom scrapy.exceptions import DropItemimport loggingimport pymongofrom .items import KeywordspiderItemfrom scrapy.conf import settingsclass SespiderPipeline(object):    # def __init__(self):    #     self.file = open('urls.txt', 'w', encoding='utf-8')    #     # self.bloomFilter = rBloomFilter.rBloomFilter(100000, 0.01, 'bing')    #    # def process_item(self, item, spider):    #     # self.file.write(item['title'] + '\n')    #     self.file.write(item['url'] + '\n' + item['title'] + '\n' + item['time'] + '\n' + item['abstract'] + '\n')    #     return item    def __init__(self):        self.client = pymongo.MongoClient(host=settings['MONGO_HOST'], port=settings['MONGO_PORT'])        self.db = self.client[settings['MONGO_DB']]        self.baidu = self.db['baidu']    def process_item(self, item, spider):        if isinstance(item, KeywordspiderItem):            try:                if item['title']:                    item = dict(item)                    self.baidu.insert(item)                    print('insert baidu success!')                    return item                else:                    pass            except Exception as e:                spider.logger.exception('insert failed!')